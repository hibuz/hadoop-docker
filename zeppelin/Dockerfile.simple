# == Info =======================================
# hibuz/bash==hibuz/hadoop-base(SIZE: 292MB) -> hibuz/zeppelin-dev(SIZE: 4.53GB)

# == Build ======================================
# docker build -t hibuz/zeppelin-dev:simple -f Dockerfile.simple .
# or
# docker build -t hibuz/zeppelin-dev:simple --build-arg ZEPPELIN_VERSION=x.y.z -f Dockerfile.simple .

# == Run and Attatch ============================
# docker run --rm -it -p 8081:8081 -p 9995:9995 -p 18080:18080 --name zeppelin-tmp hibuz/zeppelin-dev:simple
# 
# docker exec -it zeppelin-tmp bash


# == Init =======================================
FROM hibuz/hadoop-base
LABEL org.opencontainers.image.authors="hibuz@hibuz.com"

# == Package Setting ============================
ARG JDK_VERSION=11
RUN sudo apt update && DEBIAN_FRONTEND=noninteractive sudo apt install -y --no-install-recommends \
  openjdk-${JDK_VERSION}-jdk-headless \
  tini \
  ssh \
  netcat-openbsd \
  libsnappy-dev \
  && sudo rm -rf /var/lib/apt/lists/* \
  && sudo apt autoclean \
  && sudo apt clean

# == Install ============================
ARG SPARK_VERSION=3.5.6
ENV SPARK_HOME=/home/${DEFAULT_USER}/spark-${SPARK_VERSION}

RUN set -x \
    && DOWNLOAD_URL="https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz" \
    && curl -fSL "$DOWNLOAD_URL" -o download.tar.gz \
    && tar -xvf download.tar.gz \
    && mv spark-${SPARK_VERSION}-bin-* $SPARK_HOME \
    && mkdir /tmp/spark-events \
    && rm download.tar.gz

ARG ZEPPELIN_VERSION=0.12.0
ENV ZEPPELIN_HOME=/home/${DEFAULT_USER}/zeppelin-${ZEPPELIN_VERSION}

RUN set -x \
    && DOWNLOAD_URL="https://dlcdn.apache.org/zeppelin/zeppelin-${ZEPPELIN_VERSION}/zeppelin-${ZEPPELIN_VERSION}-bin-all.tgz" \
    && curl -fSL "$DOWNLOAD_URL" -o download.tar.gz \
    && tar -xvf download.tar.gz \
    && mv zeppelin-${ZEPPELIN_VERSION}-bin-all $ZEPPELIN_HOME \
    && rm download.tar.gz

ARG FLINK_VERSION=1.17.2
ENV FLINK_HOME=/home/${DEFAULT_USER}/flink-${FLINK_VERSION}

RUN set -x \
    && DOWNLOAD_URL="https://dlcdn.apache.org/flink/flink-${FLINK_VERSION}/flink-${FLINK_VERSION}-bin-scala_2.12.tgz" \
    && curl -fSL "$DOWNLOAD_URL" -o download.tar.gz \
    && tar -xvf download.tar.gz \
    && rm download.tar.gz

ARG MINICONDA_VERSION=py310_25.7.0-2
ENV CONDA_HOME=/opt/conda

RUN set -x \
    && DOWNLOAD_URL="https://repo.anaconda.com/miniconda/Miniconda3-${MINICONDA_VERSION}-Linux-x86_64.sh" \
    && curl -fSL "$DOWNLOAD_URL" -o miniconda.sh \
    && sudo bash miniconda.sh -b -p ${CONDA_HOME} \
    && ${CONDA_HOME}/bin/python -m pip install --upgrade pip \
    && rm miniconda.sh

# == Env Setting ============================
ENV PATH=$HOME/.local/bin:$PATH:$CONDA_HOME/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$FLINK_HOME/bin:$ZEPPELIN_HOME/bin
ENV ZEPPELIN_ADDR=0.0.0.0
ENV ZEPPELIN_PORT=9995
ENV USE_HADOOP=false
ENV DISABLE_JEMALLOC=true

# flink-connector-<NAME> which is a thin JAR including only the connector code, but excluding eventual third-party dependencies
# flink-sql-connector-<NAME> which is an uber JAR ready to use with all the connector third-party dependencies
RUN echo "export HADOOP_CLASSPATH=\$(\$HADOOP_HOME/bin/hadoop classpath)" >> $FLINK_HOME/bin/config.sh \
    && cd $FLINK_HOME/lib \
    && curl -O "https://repo1.maven.org/maven2/org/apache/flink/flink-table-api-java-bridge/${FLINK_VERSION}/flink-table-api-java-bridge-${FLINK_VERSION}.jar" \
    && curl -O "https://repo1.maven.org/maven2/org/apache/flink/flink-table-api-scala-bridge_2.12/${FLINK_VERSION}/flink-table-api-scala-bridge_2.12-${FLINK_VERSION}.jar" \
    && curl -O "https://repo1.maven.org/maven2/org/apache/flink/flink-csv/${FLINK_VERSION}/flink-csv-${FLINK_VERSION}.jar" \
    && curl -O "https://repo1.maven.org/maven2/org/apache/flink/flink-json/${FLINK_VERSION}/flink-json-${FLINK_VERSION}.jar" \
    && curl -O "https://repo1.maven.org/maven2/org/apache/flink/flink-parquet/${FLINK_VERSION}/flink-parquet-${FLINK_VERSION}.jar" \
    && curl -O "https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-hive-3.1.3_2.12/${FLINK_VERSION}/flink-sql-connector-hive-3.1.3_2.12-${FLINK_VERSION}.jar" \
    && curl -O "https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-hbase-2.2/4.0.0-1.19/flink-sql-connector-hbase-2.2-4.0.0-1.19.jar" \
    && cp ../opt/flink-sql-*.jar ../opt/flink-python-*.jar ./ 
    
RUN pip install jupyter grpcio protobuf==3.19.6 pandas apache-flink==${FLINK_VERSION}

COPY docker-entrypoint.sh /

WORKDIR ${ZEPPELIN_HOME}

EXPOSE 4040 8081 $ZEPPELIN_PORT

ENTRYPOINT $ZEPPELIN_HOME/bin/zeppelin-daemon.sh start && bash
